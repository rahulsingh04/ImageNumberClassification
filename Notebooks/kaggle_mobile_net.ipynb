{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7727526,"sourceType":"datasetVersion","datasetId":4514884}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! unzip /kaggle/input/numerical-image-dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.compat.v1 import ConfigProto\n# from tensorflow.compat.v1 import InteractiveSession\n\n# config = ConfigProto()\n# config.gpu_options.per_process_gpu_memory_fraction = 0.5 ## only use 50 % of the gpu\n# config.gpu_options.allow_growth = True\n# session = InteractiveSession(config=config)\n\nimport tensorflow as tf\n\ndevice_name = tf.test.gpu_device_name()\n\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\n    \nprint('Found GPU at: {}'.format(device_name))\n\nprint(\"GPU\", \"available (YESS!!!!)\" if tf.config.list_physical_devices(\"GPU\") else \"not available :(\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:47:53.466695Z","iopub.execute_input":"2024-02-29T17:47:53.467431Z","iopub.status.idle":"2024-02-29T17:48:08.098061Z","shell.execute_reply.started":"2024-02-29T17:47:53.467402Z","shell.execute_reply":"2024-02-29T17:48:08.097074Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-29 17:47:55.680551: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-29 17:47:55.680722: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-29 17:47:55.846471: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Found GPU at: /device:GPU:0\nGPU available (YESS!!!!)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"###  Importing Necessary Library ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom glob import glob\nfrom tensorflow.keras.layers import (Input, Lambda, Dense, Flatten, Conv2D)\nfrom tensorflow.keras.models import (Model, Sequential)\nfrom tensorflow.keras.applications.mobilenet_v2 import ( MobileNetV2, preprocess_input)\nfrom tensorflow.keras.preprocessing.image import (ImageDataGenerator, load_img, array_to_img, img_to_array)\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.applications.resnet50 import ResNet50\n#from keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.resnet50 import preprocess_input","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:08.099953Z","iopub.execute_input":"2024-02-29T17:48:08.100980Z","iopub.status.idle":"2024-02-29T17:48:08.120225Z","shell.execute_reply.started":"2024-02-29T17:48:08.100944Z","shell.execute_reply":"2024-02-29T17:48:08.119069Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# re-size all the images to this\nIMAGE_SIZE = [224, 224]\n\ntrain_path = '/kaggle/input/numerical-image-dataset/dataset/train'\nvalid_path = '/kaggle/input/numerical-image-dataset/dataset/val'\ntest_path =  \"/kaggle/input/numerical-image-dataset/dataset/test\"","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:08.121819Z","iopub.execute_input":"2024-02-29T17:48:08.122202Z","iopub.status.idle":"2024-02-29T17:48:08.139979Z","shell.execute_reply.started":"2024-02-29T17:48:08.122168Z","shell.execute_reply":"2024-02-29T17:48:08.139066Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Import the MobileNet library as shown below and add preprocessing layer to the front of MobileNet\n# Here we will be using imagenet weights\n\nmobile_net  = MobileNetV2(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False,\n                         pooling='avg')\n\n# resnet = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:08.142520Z","iopub.execute_input":"2024-02-29T17:48:08.142803Z","iopub.status.idle":"2024-02-29T17:48:09.775268Z","shell.execute_reply.started":"2024-02-29T17:48:08.142757Z","shell.execute_reply":"2024-02-29T17:48:09.774460Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# mobile_net.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:09.776458Z","iopub.execute_input":"2024-02-29T17:48:09.776790Z","iopub.status.idle":"2024-02-29T17:48:09.780907Z","shell.execute_reply.started":"2024-02-29T17:48:09.776751Z","shell.execute_reply":"2024-02-29T17:48:09.779979Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(\"No Of The Layer In MObileNet :--> \", len(mobile_net.layers))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:09.782082Z","iopub.execute_input":"2024-02-29T17:48:09.782358Z","iopub.status.idle":"2024-02-29T17:48:09.846145Z","shell.execute_reply.started":"2024-02-29T17:48:09.782335Z","shell.execute_reply":"2024-02-29T17:48:09.845216Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"No Of The Layer In MObileNet :-->  155\n","output_type":"stream"}]},{"cell_type":"code","source":"  # useful for getting number of output classes\nfolders = glob('/kaggle/input/numerical-image-dataset/dataset/train/*')\nprint(\"total no of the folder in the train dataset :-->\", len(folders))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:09.847359Z","iopub.execute_input":"2024-02-29T17:48:09.847708Z","iopub.status.idle":"2024-02-29T17:48:09.878961Z","shell.execute_reply.started":"2024-02-29T17:48:09.847673Z","shell.execute_reply":"2024-02-29T17:48:09.878094Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"total no of the folder in the train dataset :--> 100\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Use the Image Data Generator to import the images from the dataset\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n\n# Define data generators for training and validation data\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,            # Rescale pixel values to the range [0, 1]\n    preprocessing_function=preprocess_input,\n    rotation_range=40,         # Randomly rotate images by up to 40 degrees\n    width_shift_range=0.3,     # Randomly shift images horizontally by up to 30% of the width\n    height_shift_range=0.3,    # Randomly shift images vertically by up to 30% of the height\n    shear_range=0.3,           # Shear intensity\n    zoom_range=0.3,            # Randomly zoom images by up to 30%\n    horizontal_flip=True,      # Randomly flip images horizontally\n    vertical_flip=True,        # Randomly flip images vertically\n    brightness_range=[0.6, 1.4],  # Randomly adjust brightness\n    channel_shift_range=100.0,  # Randomly shift channels\n    fill_mode='nearest',       # Strategy for filling in newly created pixels\n    validation_split=0.2       # Fraction of training data to use for validation\n)\n\ntest_datagen = ImageDataGenerator(rescale=1./255,   # Rescale pixel values for the test set\n                                  preprocessing_function=preprocess_input)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:09.879875Z","iopub.execute_input":"2024-02-29T17:48:09.880107Z","iopub.status.idle":"2024-02-29T17:48:09.886677Z","shell.execute_reply.started":"2024-02-29T17:48:09.880086Z","shell.execute_reply":"2024-02-29T17:48:09.885959Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Make sure to provide the same target size as initialied for the image size\n\ntraining_set = train_datagen.flow_from_directory(train_path,\n                                                 target_size = (224, 224),\n                                                 batch_size = 32,\n                                                 class_mode = 'categorical')\n\nvalidation_set = test_datagen.flow_from_directory(valid_path,\n                                            target_size = (224, 224),\n                                            batch_size = 32,\n                                            class_mode = 'categorical')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:09.888023Z","iopub.execute_input":"2024-02-29T17:48:09.888311Z","iopub.status.idle":"2024-02-29T17:48:12.267844Z","shell.execute_reply.started":"2024-02-29T17:48:09.888287Z","shell.execute_reply":"2024-02-29T17:48:12.267060Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Found 25300 images belonging to 100 classes.\nFound 3200 images belonging to 100 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"LIST OF THE CLASS NAME  ARE :-\", list(training_set.class_indices.keys()))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:12.270616Z","iopub.execute_input":"2024-02-29T17:48:12.270926Z","iopub.status.idle":"2024-02-29T17:48:12.275601Z","shell.execute_reply.started":"2024-02-29T17:48:12.270901Z","shell.execute_reply":"2024-02-29T17:48:12.274619Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"LIST OF THE CLASS NAME  ARE :- ['1', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Let's take a look to see how many layers are in the base model\nprint(\"Number of layers in the base model: \", len(mobile_net.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = 150\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in mobile_net.layers[:fine_tune_at]:\n    layer.trainable =  False","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:12.276749Z","iopub.execute_input":"2024-02-29T17:48:12.277451Z","iopub.status.idle":"2024-02-29T17:48:12.289907Z","shell.execute_reply.started":"2024-02-29T17:48:12.277427Z","shell.execute_reply":"2024-02-29T17:48:12.289013Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Number of layers in the base model:  155\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n#learning_rate reduce module\nlr_reduce = ReduceLROnPlateau('val_loss', patience=4,\n                                              factor=0.5, min_lr=1e-6)\n\n# Stop early if model doesn't improve after n epochs\nearly_stopper = EarlyStopping(monitor='val_loss', patience=4,\n                              verbose=0, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:12.290950Z","iopub.execute_input":"2024-02-29T17:48:12.291226Z","iopub.status.idle":"2024-02-29T17:48:12.301897Z","shell.execute_reply.started":"2024-02-29T17:48:12.291178Z","shell.execute_reply":"2024-02-29T17:48:12.301113Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import (\n    LeakyReLU, PReLU, ReLU, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n)\n\nmodel = Sequential()\n\nmodel.add(mobile_net)\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dense(len(folders), activation  = 'softmax'))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:12.302869Z","iopub.execute_input":"2024-02-29T17:48:12.303123Z","iopub.status.idle":"2024-02-29T17:48:12.333678Z","shell.execute_reply.started":"2024-02-29T17:48:12.303101Z","shell.execute_reply":"2024-02-29T17:48:12.332927Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ mobilenetv2_1.00_224            │ ?                      │     \u001b[38;5;34m2,257,984\u001b[0m │\n│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ mobilenetv2_1.00_224            │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m412,800\u001b[0m (1.57 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">412,800</span> (1.57 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,845,184\u001b[0m (7.04 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,845,184</span> (7.04 MB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:12.334536Z","iopub.execute_input":"2024-02-29T17:48:12.334800Z","iopub.status.idle":"2024-02-29T17:48:12.357699Z","shell.execute_reply.started":"2024-02-29T17:48:12.334756Z","shell.execute_reply":"2024-02-29T17:48:12.356923Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ mobilenetv2_1.00_224            │ ?                      │     \u001b[38;5;34m2,257,984\u001b[0m │\n│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ mobilenetv2_1.00_224            │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m412,800\u001b[0m (1.57 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">412,800</span> (1.57 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,845,184\u001b[0m (7.04 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,845,184</span> (7.04 MB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"model.compile(\n  loss='categorical_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:12.358630Z","iopub.execute_input":"2024-02-29T17:48:12.358894Z","iopub.status.idle":"2024-02-29T17:48:12.372718Z","shell.execute_reply.started":"2024-02-29T17:48:12.358871Z","shell.execute_reply":"2024-02-29T17:48:12.372050Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"history_fine = model.fit(training_set,\n                         batch_size = 32,\n                         epochs=500,\n                         validation_data=validation_set,\n                         callbacks=[lr_reduce, early_stopper],\n                         verbose=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:48:12.373696Z","iopub.execute_input":"2024-02-29T17:48:12.373990Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/500\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  1/791\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:23:58\u001b[0m 20s/step - accuracy: 0.0000e+00 - loss: 4.7024","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1709228913.010090     104 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1709228913.042122     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m594/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 533ms/step - accuracy: 0.0083 - loss: 4.6278","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1709229229.045027     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m790/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 527ms/step - accuracy: 0.0082 - loss: 4.6234","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1709229332.515388     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 553ms/step - accuracy: 0.0082 - loss: 4.6233 - val_accuracy: 0.0100 - val_loss: 4.6054 - learning_rate: 0.0010\nEpoch 2/500\n\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 413ms/step - accuracy: 0.0084 - loss: 4.6056 - val_accuracy: 0.0100 - val_loss: 4.6055 - learning_rate: 0.0010\nEpoch 3/500\n\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 405ms/step - accuracy: 0.0071 - loss: 4.6056 - val_accuracy: 0.0100 - val_loss: 4.6052 - learning_rate: 0.0010\nEpoch 4/500\n\u001b[1m648/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m56s\u001b[0m 398ms/step - accuracy: 0.0083 - loss: 4.6056","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# plot the loss\nplt.plot(history_fine.history['loss'], label='train loss')\nplt.plot(history_fine.history['val_loss'], label='val loss')\nplt.legend()\nplt.show()\nplt.savefig('LossVal_loss')\n\n# plot the accuracy\nplt.plot(history_fine.history['accuracy'], label='train acc')\nplt.plot(history_fine.history['val_accuracy'], label='val acc')\nplt.legend()\nplt.show()\nplt.savefig('AccVal_acc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save it as a h5 file\n\n\nfrom tensorflow.keras.models import load_model\n\nmodel.save('resnet.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.argmax(model.predict(validation_set), axis =1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\n\nmodel = load_model('resnet.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img=image.load_img('/kaggle/input/numerical-image-dataset/dataset/test/1/image_103.png',target_size=(224,224))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=image.img_to_array(img)\nx=x/255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nx=np.expand_dims(x,axis=0)\nimg_data=preprocess_input(x)\nimg_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict(img_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=np.argmax(model.predict(img_data), axis=1)\na","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}